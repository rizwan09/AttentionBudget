
### Rationalizing Neural Predictions, Tao Lei et al., EMNLP, 2016
Abstract:
 
This paper learns to extract pieces of input text as justifications – rationales – that are tailored to be short and coherent,
yet sufficient for making prediction. It combines two modular components, generator and encoder, which are trained to operate 
well together. The generator specifies a distribution over text fragments as candidate rationales and these are passed through
the encoder for prediction. Rationales are never given during training. Instead, the model is regularized by desiderata for 
rationales.
###![image](https://user-images.githubusercontent.com/6012792/28242041-5fea33de-696f-11e7-99e7-1385e1d291fc.png)
